[2016/08/15]

Problem:
--------

"
Fatal error:
cudaMallocHost of size 1024128 bytes failed: all CUDA-capable devices are busy or unavailable
"

Identifying faulty C-nodes:
---------------------------

- ran "./cluster_fork.py -c -L -s nvidia-smi" => found out that c012 has only 3 GPUs that work 
- testing running job with only one node and sbatch argument --nodelist on each node separately and check whether it fails or not. Failing nodes:
  c003 OK
  c004 OK
  c005 OK
  c006 failed
  c007 OK
  c008 OK
  c009 OK
  c010 OK
  c011 OK
  c012 only 3 GPUs, see above 
  c013 OK 
  c014 OK
  c015 OK

Minimization:
-------------

- job running on 10 well-behaving nodes succeeds:
"
#!/bin/bash
#SBATCH --nodes=10
#SBATCH --nodelist=c003,c004,c005,c007,c008,c009,c010,c011,c013,c014
#SBATCH --ntasks-per-node=12
#SBATCH --gres=gpu:4
#SBATCH --exclusive
#SBATCH --partition=c
#SBATCH --exclude=c002,c012
#SBATCH --job-name=gmx_min
#SBATCH --output=out/gmx_min.out
cd out
mpirun gmx_mpi mdrun -v -deffnm em
"

output:
"
Step= 5325, Dmax= 3.4e-03 nm, Epot= -4.97928e+06 Fmax= 9.24112e+02, atom= 111

writing lowest energy coordinates.

Steepest Descents converged to Fmax < 1000 in 5326 steps
Potential Energy  = -4.9792795e+06
Maximum force     =  9.2411206e+02 on atom 111
Norm of force     =  1.0198773e+01

NOTE: 13 % of the run time was spent in domain decomposition,
      4 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)
"

Equilibration:
--------------

"
#!/bin/bash
#SBATCH --nodes=10
#SBATCH --nodelist=c003,c004,c005,c007,c008,c009,c010,c013,c014,c015
#SBATCH --ntasks-per-node=6
#SBATCH --cpus-per-task=4
#SBATCH --gres=gpu:3
#SBATCH --partition=c
#SBATCH --exclude=c002,c012
#SBATCH --job-name=gmx_eq1
#SBATCH --output=out/gmx_eq1.out
export OMP_NUM_THREADS=4
cd out/
mpirun gmx_mpi mdrun -deffnm nvt -pin on -ntomp 4 -gpu_id 001122
"

Segfault:
"
mpirun noticed that process rank 47 with PID 29167 on node c011 exited on signal 11 (Segmentation fault).
"

- Same result when not running with c011 and c015 instead
- Same result with configuration used for minimization
- New result with following configuration:

"
#!/bin/bash
#SBATCH --nodes=10
#SBATCH --nodelist=c003,c004,c005,c007,c008,c009,c010,c013,c014,c015
#SBATCH --ntasks-per-node=12
#SBATCH --gres=gpu:4
#SBATCH --partition=c
#SBATCH --exclusive
#SBATCH --exclude=c002,c012
#SBATCH --job-name=gmx_eq1
#SBATCH --output=out/gmx_eq1.out
cd out/
mpirun gmx_mpi mdrun -deffnm nvt
"

Segfault with verbos error message:
"
--------------------------------------------------------------------------
The InfiniBand retry count between two MPI processes has been
exceeded.  "Retry count" is defined in the InfiniBand spec 1.2
(section 12.7.38):

    The total number of times that the sender wishes the receiver to
    retry timeout, packet sequence, etc. errors before posting a
    completion error.

This error typically means that there is something awry within the
InfiniBand fabric itself.  You should note the hosts on which this
error has occurred; it has been observed that rebooting or removing a
particular host from the job can sometimes resolve this issue.

Two MCA parameters can be used to control Open MPI's behavior with
respect to the retry count:

* btl_openib_ib_retry_count - The number of times the sender will
  attempt to retry (defaulted to 7, the maximum value).
* btl_openib_ib_timeout - The local ACK timeout parameter (defaulted
  to 20).  The actual timeout value used is calculated as:

     4.096 microseconds * (2^btl_openib_ib_timeout)

  See the InfiniBand spec 1.2 (section 12.7.34) for more details.

Below is some information about the host that raised the error and the
peer to which it was connected:

  Local host:   c007
  Local device: mlx4_0
  Peer host:    c010

You may need to consult with your system administrator to get this
problem fixed.
--------------------------------------------------------------------------
[[3549,1],47][btl_openib_component.c:3369:handle_wc] from c007 to: c010 error polling LP CQ with status RETRY EXCEEDED ERROR status number 12 for wr_id 137b880 opcode 1  vendor error 129 qp_idx 1
--------------------------------------------------------------------------
mpirun noticed that process rank 83 with PID 16698 on node c010 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
"

- try without ib:
"
#!/bin/bash
#SBATCH --nodes=10
#SBATCH --nodelist=c003,c004,c005,c007,c008,c009,c010,c013,c014,c015
#SBATCH --ntasks-per-node=12
#SBATCH --gres=gpu:4
#SBATCH --partition=c
#SBATCH --exclusive
#SBATCH --exclude=c002,c012
#SBATCH --job-name=gmx_eq1
#SBATCH --output=out/gmx_eq1.out
cd out/
mpirun --mca btl ^openib gmx_mpi mdrun -deffnm nvt
"

Fails with:
"
[c013][[3839,1],91][btl_tcp_frag.c:215:mca_btl_tcp_frag_recv] mca_btl_tcp_frag_recv: readv failed: Connection reset by peer (104)
--------------------------------------------------------------------------
mpirun noticed that process rank 103 with PID 26785 on node c014 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
"

- try without GPUs:
"
##SBATCH --gres=gpu:4
"

Also segfaults:
"
--------------------------------------------------------------------------
mpirun noticed that process rank 90 with PID 31724 on node c013 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
"

- try with longer MPI timeout and without GPUs:
"
mpirun --mca btl_openib_ib_timeout 24 gmx_mpi mdrun -deffnm nvt
"

Also segfaults:
"
--------------------------------------------------------------------------
mpirun noticed that process rank 112 with PID 30184 on node c015 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
"

Try with less processes same nodes:
"
#!/bin/bash
#SBATCH --nodes=7
#SBATCH --nodelist=c003,c004,c005,c007,c008,c009,c010,c013,c014,c015
#SBATCH --ntasks-per-node=4
#SBATCH --partition=c
#SBATCH --exclusive
#SBATCH --exclude=c002,c012
#SBATCH --job-name=gmx_eq1
#SBATCH --output=out/gmx_eq1.out
export OMP_NUM_THREADS=6
cd out/
mpirun gmx_mpi mdrun -deffnm nvt
"

Segfault:
"
--------------------------------------------------------------------------
mpirun noticed that process rank 38 with PID 30462 on node c015 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
"

Try without nodes that showed up with segfault:
"
#!/bin/bash
#SBATCH --nodes=7
#SBATCH --nodelist=c003,c004,c005,c008,c009,c013,c014
#SBATCH --ntasks-per-node=4
#SBATCH --partition=c
#SBATCH --exclusive
#SBATCH --exclude=c002,c012
#SBATCH --job-name=gmx_eq1
#SBATCH --output=out/gmx_eq1.out
export OMP_NUM_THREADS=6
cd out/
mpirun gmx_mpi mdrun -deffnm nvt
"

Segfaults:
"
mpirun noticed that process rank 2 with PID 23574 on node c003 exited on signal 11 (Segmentation fault).
"

2016/08/18

Using newly compiled GROMACS 2016 (on node c022)

Only using one GPU for equilibration nvt works.

Input:
"
#!/bin/bash
#SBATCH --exclude=c002,c012
#SBATCH --nodelist=c022
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --gres=gpu:1
#SBATCH --partition=c
#SBATCH --exclusive
#SBATCH --job-name=gmx
#SBATCH --output=/home/hantke/complex/md/03_droplet/test2016/run.log
#SBATCH --workdir=/home/hantke/complex/md/03_droplet/test2016
source ./env
cd out/
gmx mdrun -deffnm nvt -v 
"

Output:
"
Using 1 MPI thread
Using 24 OpenMP threads 

1 compatible GPU is present, with ID 0
1 GPU auto-selected for this run.
Mapping of GPU ID to the 1 PP rank in this node: 0


Back Off! I just backed up nvt.trr to ./#nvt.trr.21#

Back Off! I just backed up nvt.edr to ./#nvt.edr.22#

WARNING: This run will generate roughly 6895 Mb of data

starting mdrun 'UBIQUITIN in water'
50000 steps,    200.0 ps.
step   80: timed with pme grid 160 160 160, coulomb cutoff 1.000: 6221.1 M-cycles
step  160: timed with pme grid 128 128 128, coulomb cutoff 1.221: 4533.9 M-cycles
step  240: timed with pme grid 112 112 112, coulomb cutoff 1.395: 3998.3 M-cycles
step  320: timed with pme grid 100 100 100, coulomb cutoff 1.562: 3743.8 M-cycles
step  400: timed with pme grid 84 84 84, coulomb cutoff 1.860: 3430.0 M-cycles
step  480: timed with pme grid 72 72 72, coulomb cutoff 2.170: 4214.7 M-cycles
step  560: timed with pme grid 80 80 80, coulomb cutoff 1.953: 3512.9 M-cycles
step  640: timed with pme grid 84 84 84, coulomb cutoff 1.860: 3613.2 M-cycles
step  720: timed with pme grid 96 96 96, coulomb cutoff 1.628: 3708.7 M-cycles
step  800: timed with pme grid 100 100 100, coulomb cutoff 1.562: 3613.8 M-cycles
step  880: timed with pme grid 104 104 104, coulomb cutoff 1.502: 3905.3 M-cycles
step  960: timed with pme grid 108 108 108, coulomb cutoff 1.447: 4056.7 M-cycles
step 1040: timed with pme grid 80 80 80, coulomb cutoff 1.953: 3597.3 M-cycles
step 1120: timed with pme grid 84 84 84, coulomb cutoff 1.860: 3626.3 M-cycles
step 1200: timed with pme grid 96 96 96, coulomb cutoff 1.628: 3565.4 M-cycles
step 1280: timed with pme grid 100 100 100, coulomb cutoff 1.562: 3821.3 M-cycles
step 1360: timed with pme grid 104 104 104, coulomb cutoff 1.502: 3931.0 M-cycles
step 1440: timed with pme grid 108 108 108, coulomb cutoff 1.447: 4019.7 M-cycles
step 1520: timed with pme grid 80 80 80, coulomb cutoff 1.953: 3948.7 M-cycles
step 1600: timed with pme grid 84 84 84, coulomb cutoff 1.860: 3475.5 M-cycles
step 1680: timed with pme grid 96 96 96, coulomb cutoff 1.628: 3732.3 M-cycles
step 1760: timed with pme grid 100 100 100, coulomb cutoff 1.562: 3823.3 M-cycles
              optimal pme grid 84 84 84, coulomb cutoff 1.860
step 49900, remaining wall clock time:     4 s          
Writing final coordinates.
step 50000, remaining wall clock time:     0 s          
               Core t (s)   Wall t (s)        (%)
       Time:    52775.503     2198.979     2400.0
                         36:38
                 (ns/day)    (hour/ns)
Performance:        7.858        3.054
"

